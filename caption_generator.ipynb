{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58889d38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import string\n",
    "from pickle import dump, load\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af03ad02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.xception import Xception \n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2f11db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path: str):\n",
    "    file = open(file_path, \"r\")\n",
    "    text = file.readlines()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db55a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def develop_caption(file_path: str):\n",
    "    texts = read_file(file_path)\n",
    "    descriptions = {}\n",
    "    for i in range(len(texts)):\n",
    "        filename, caption = texts[i].split(\"\\t\")\n",
    "        filename, _ = filename.split(\"#\")\n",
    "        try:\n",
    "            _ = len(descriptions[filename])\n",
    "        except:\n",
    "            descriptions[filename] = []\n",
    "        descriptions[filename].append(caption.lower())\n",
    "    return descriptions\n",
    "\n",
    "descriptions = develop_caption(\"data\\\\labels\\\\Flickr8k.token.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37e2c2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(captions: dict):\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    for filename, caps in captions.items():\n",
    "        for i, label in enumerate(caps):\n",
    "            label.replace(\"-\",\" \")\n",
    "            label = label.split()\n",
    "            #uppercase to lowercase\n",
    "            label = [word.lower() for word in label]\n",
    "            #remove punctuation from each token\n",
    "            label = [word.translate(table) for word in label]\n",
    "            #remove hanging 's and a\n",
    "            label = [word for word in label if(len(word) > 1)]\n",
    "            #remove words containing numbers with them\n",
    "            label = [word for word in label if(word.isalpha())]\n",
    "            #converting back to string\n",
    "            label = \" \".join(label)\n",
    "            captions[filename][i]= label\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b766468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabulary(captions: dict):\n",
    "    # To build vocabulary of all unique words\n",
    "    vocab = set()\n",
    "    for key in descriptions.keys():\n",
    "        [vocab.update(d.split()) for d in captions[key]]\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1479943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_captions(captions: dict, file_path: str):\n",
    "    lines = list()\n",
    "    for key, caption_list in captions.items():\n",
    "        for caption in caption_list:\n",
    "            lines.append(key + \"\\t\" + caption)\n",
    "    data = \"\\n\".join(lines)\n",
    "    file = open(file_path, \"w\")\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caa5262e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of captions: 8092\n",
      "Unique words in vocabulary: 8763\n",
      "saved\n"
     ]
    }
   ],
   "source": [
    "dataset_text = \"data\\\\labels\"\n",
    "dataset_images = \"data\\\\images\"\n",
    "\n",
    "#to prepare our text data\n",
    "file_name = os.path.join(dataset_text, \"Flickr8k.token.txt\")\n",
    "\n",
    "#loading the file that contains all data\n",
    "#map them into descriptions dictionary \n",
    "captions = develop_caption(file_name)\n",
    "print(\"Length of captions:\", len(captions))\n",
    "\n",
    "#cleaning the descriptions\n",
    "clean_captions = clean_text(captions)\n",
    "\n",
    "#to build vocabulary\n",
    "vocab = vocabulary(clean_captions)\n",
    "print(\"Unique words in vocabulary:\", len(vocab))\n",
    "\n",
    "#saving all descriptions in one file\n",
    "save_captions(clean_captions, \"captions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad9e513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_image_features(image_folder_path):\n",
    "#     xception_model = Xception(include_top = False, pooling = \"avg\")\n",
    "#     features = {}\n",
    "#     image_files = [os.path.join(image_folder_path, f) for f in os.listdir(image_folder_path)]\n",
    "#     for i in tqdm(range(len(image_files))):\n",
    "#         image_path = image_files[i]\n",
    "#         image = Image.open(image_path)\n",
    "#         image = image.resize((299,299))\n",
    "#         image = np.expand_dims(image, axis=0)\n",
    "#         image = image/127.5\n",
    "#         image = image - 1.0\n",
    "#         feature = xception_model.predict(image, verbose=False)\n",
    "#         features[image_path] = feature\n",
    "#     return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51c3f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # features = extract_image_features(dataset_images)\n",
    "# # dump(features, open(\"features.p\", \"wb\"))\n",
    "\n",
    "# #to directly load the features from the pickle file.\n",
    "# features = load(open(\"features.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ee983c",
   "metadata": {},
   "source": [
    "### Loading dataset for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0f07453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_photos(file_path: str):\n",
    "#     file = read_file(file_path)\n",
    "#     file = [f.rstrip(\"\\n\") for f in file]\n",
    "#     return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c37b0dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_clean_captions(file_path: str, photos):\n",
    "#     #loading clean_descriptions\n",
    "#     file = read_file(file_path)\n",
    "#     captions = {}\n",
    "#     for line in file:\n",
    "#         words = line.split()\n",
    "#         if len(words)<1 :\n",
    "#             continue\n",
    "#         image, image_caption = words[0], words[1:]\n",
    "#         if image in photos:\n",
    "#             if image not in captions:\n",
    "#                 captions[image] = []\n",
    "#             caption = ' ' + \" \".join(image_caption) + ' '\n",
    "#             captions[image].append(caption)\n",
    "#     return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da65cea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_features(photos):\n",
    "#     #loading all features\n",
    "#     all_features = load(open(\"features.p\",\"rb\"))\n",
    "#     #selecting only needed features\n",
    "#     new_keys = [k.split(\"\\\\\")[-1] for k in all_features.keys()]\n",
    "#     f = {}\n",
    "#     for i, key in enumerate(all_features.keys()):\n",
    "#         f[new_keys[i]] = all_features[key]\n",
    "    \n",
    "#     features = {k:f[k] for k in photos}\n",
    "#     return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33c1839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = os.path.join(dataset_text, \"Flickr_8k.trainImages.txt\")\n",
    "# #train = loading_data(filename)\n",
    "# train_imgs = load_photos(filename)\n",
    "# train_descriptions = load_clean_captions(\"captions.txt\", train_imgs)\n",
    "# train_features = load_features(train_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d823e5b",
   "metadata": {},
   "source": [
    "### Tokenizing the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffe9d84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dict_to_list(captions: dict):\n",
    "#     all_captions = []\n",
    "#     for key in descriptions.keys():\n",
    "#         [all_captions.append(d) for d in descriptions[key]]\n",
    "#     return all_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "755bc9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# def create_tokenizer(captions: dict):\n",
    "#     # Ensure dict_to_list is defined or imported if used\n",
    "#     captions_list = dict_to_list(captions)  # Assuming dict_to_list function converts dictionary values to a list\n",
    "#     tokenizer = Tokenizer()\n",
    "#     tokenizer.fit_on_texts(captions_list)\n",
    "#     return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b03f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = create_tokenizer(captions)\n",
    "# dump(tokenizer, open('tokenizer.p', 'wb'))\n",
    "# vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45c4aa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size=8494\n",
      "max_length=30\n"
     ]
    }
   ],
   "source": [
    "# def max_length(captions):\n",
    "#     captions = dict_to_list(descriptions)\n",
    "#     return max(len(d.split()) for d in captions)\n",
    "\n",
    "# max_length = 30\n",
    "# print(f\"{vocab_size=}\")\n",
    "# print(f\"{max_length=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcca546",
   "metadata": {},
   "source": [
    "### Create a Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32dc2067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_generator(descriptions, features, tokenizer, max_length):\n",
    "#     while 1:\n",
    "#         for key, description_list in descriptions.items():\n",
    "#             #retrieve photo features\n",
    "#             feature = features[key][0]\n",
    "#             inp_image, inp_seq, op_word = create_sequences(tokenizer, max_length, description_list, feature)\n",
    "#             yield [[inp_image, inp_seq], op_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de7cb328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_sequences(tokenizer, max_length, desc_list, feature):\n",
    "#     x_1, x_2, y = list(), list(), list()\n",
    "#     # move through each description for the image\n",
    "#     for desc in desc_list:\n",
    "#         # encode the sequence\n",
    "#         seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "#         # divide one sequence into various X,y pairs\n",
    "#         for i in range(1, len(seq)):\n",
    "#             # divide into input and output pair\n",
    "#             in_seq, out_seq = seq[:i], seq[i]\n",
    "#             # pad input sequence\n",
    "#             in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "#             # encode output sequence\n",
    "#             out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "#             # store\n",
    "#             x_1.append(feature)\n",
    "#             x_2.append(in_seq)\n",
    "#             y.append(out_seq)\n",
    "#     return np.array(x_1), np.array(x_2), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf9eeb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37, 2048) (37, 30) (37, 8494)\n"
     ]
    }
   ],
   "source": [
    "# #To check the shape of the input and output for your model\n",
    "# [a,b],c = next(data_generator(train_descriptions, train_features, tokenizer, max_length))\n",
    "# print(a.shape, b.shape, c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f1b15d",
   "metadata": {},
   "source": [
    "### Define the CNN-RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58eab064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import plot_model\n",
    "# from tensorflow.keras.layers import Add\n",
    "\n",
    "# def get_model(vocab_size, max_length):\n",
    "#     inputs1 = Input(shape=(2048,))\n",
    "#     fe1 = Dropout(0.5)(inputs1)\n",
    "#     fe2 = Dense(256, activation='relu')(fe1)\n",
    "#     fe3 = Dense(128, activation=\"relu\")(fe2)\n",
    "#     # LSTM sequence model\n",
    "#     inputs2 = Input(shape=(max_length,))\n",
    "#     se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "#     se2 = Dropout(0.5)(se1)\n",
    "#     se3 = LSTM(256, return_sequences=True)(se2)\n",
    "#     se4 = LSTM(128)(se3)\n",
    "#     # Merging both models\n",
    "#     decoder1 = Add()([fe3, se4])\n",
    "#     decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "#     outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "#     # merge it [image, seq] [word]\n",
    "#     model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "#     model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "#     # summarize model\n",
    "#     print(model.summary())\n",
    "#     plot_model(model, to_file='model.png', show_shapes=True)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cfc85e",
   "metadata": {},
   "source": [
    "### Training the Image Caption Generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5449d3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  6000\n",
      "Descriptions: train= 6000\n",
      "Photos: train= 6000\n",
      "Vocabulary Size: 8494\n",
      "Description Length:  30\n"
     ]
    }
   ],
   "source": [
    "# print('Dataset: ', len(train_imgs))\n",
    "# print('Descriptions: train=', len(train_descriptions))\n",
    "# print('Photos: train=', len(train_features))\n",
    "# print('Vocabulary Size:', vocab_size)\n",
    "# print('Description Length: ', max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e63fe4b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,174,464</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ not_equal (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           │\n",
       "│                               │                           │                 │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">197,120</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],                │\n",
       "│                               │                           │                 │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],             │\n",
       "│                               │                           │                 │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                  │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8494</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,182,958</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │       \u001b[38;5;34m2,174,464\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ not_equal (\u001b[38;5;33mNotEqual\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │         \u001b[38;5;34m524,544\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │         \u001b[38;5;34m525,312\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           │\n",
       "│                               │                           │                 │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │          \u001b[38;5;34m32,896\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │         \u001b[38;5;34m197,120\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],                │\n",
       "│                               │                           │                 │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],             │\n",
       "│                               │                           │                 │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │          \u001b[38;5;34m33,024\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                  │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8494\u001b[0m)              │       \u001b[38;5;34m2,182,958\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,670,318</span> (21.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,670,318\u001b[0m (21.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,670,318</span> (21.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,670,318\u001b[0m (21.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'fit_generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m      9\u001b[0m     generator \u001b[38;5;241m=\u001b[39m data_generator(train_descriptions, train_features, tokenizer, max_length)\n\u001b[1;32m---> 10\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit_generator(generator, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, steps_per_epoch\u001b[38;5;241m=\u001b[39mSTEPS, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m     model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_models/model_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'fit_generator'"
     ]
    }
   ],
   "source": [
    "# EPOCHS = 200\n",
    "# STEPS = len(train_descriptions)\n",
    "# if not os.path.exists(\"new_models\"):\n",
    "#     os.mkdir(\"new_models\")\n",
    "\n",
    "# model = get_model(vocab_size, max_length)\n",
    "\n",
    "# for i in range(EPOCHS):\n",
    "#     generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
    "#     model.fit_generator(generator, epochs=1, steps_per_epoch=STEPS, verbose=1)\n",
    "#     model.save(\"new_models/model_\" + str(i) + \".keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a866f79d",
   "metadata": {},
   "source": [
    "### Testing the Image Caption Generator model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a855c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "# import argparse\n",
    "\n",
    "\n",
    "# def extract_features(filename, model):\n",
    "#     try:\n",
    "#         image = Image.open(filename)\n",
    "#     except:\n",
    "#         print(\"ERROR: Can't open image! Ensure that image path and extension is correct\")\n",
    "#     image = image.resize((299,299))\n",
    "#     image = np.array(image)\n",
    "#     # for 4 channels images, we need to convert them into 3 channels\n",
    "#     if image.shape[2] == 4:\n",
    "#         image = image[..., :3]\n",
    "#     image = np.expand_dims(image, axis=0)\n",
    "#     image = image/127.5\n",
    "#     image = image - 1.0\n",
    "#     feature = model.predict(image)\n",
    "#     return feature\n",
    "\n",
    "\n",
    "# def word_for_id(integer, tokenizer):\n",
    "#     for word, index in tokenizer.word_index.items():\n",
    "#         if index == integer:\n",
    "#             return word\n",
    "#     return None\n",
    "\n",
    "\n",
    "# def generate_desc(model, tokenizer, photo, max_length):\n",
    "#     in_text = 'start'\n",
    "#     for i in range(max_length):\n",
    "#         sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "#         sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "#         pred = model.predict([photo,sequence], verbose=0)\n",
    "#         pred = np.argmax(pred)\n",
    "#         word = word_for_id(pred, tokenizer)\n",
    "#         if word is None:\n",
    "#             break\n",
    "#         in_text += ' ' + word\n",
    "#         if word == 'end':\n",
    "#             break\n",
    "#     return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d4955b-70fd-4163-8c6f-1ac8215fbebf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If using TensorFlow 2.x\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6659a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# img_path = [\"data\\\\images\\\\3385593926_d3e9c21170.jpg\", \"data\\\\images\\\\109823395_6fb423a90f.jpg\", \n",
    "#         \"data\\\\images\\\\118309463_a532b75be9.jpg\", \"data\\\\images\\\\161669933_3e7d8c7e2c.jpg\",\n",
    "#            \"data\\\\images\\\\229951087_4c20600c32.jpg\",\"data\\\\images\\\\free-images.jpg\"]\n",
    "\n",
    "# max_length = 30\n",
    "# tokenizer = load(open(\"tokenizer.p\",\"rb\"))\n",
    "# model = load_model('new_models/model_138.keras')\n",
    "# xception_model = Xception(include_top=False, pooling=\"avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187eba6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# photo = extract_features(img_path[0], xception_model)\n",
    "# img = Image.open(img_path[0])\n",
    "# description1 = generate_desc(model, tokenizer, photo, max_length)\n",
    "# print(description1)\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc153769",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# photo = extract_features(img_path[1], xception_model)\n",
    "# img = Image.open(img_path[1])\n",
    "# description2 = generate_desc(model, tokenizer, photo, max_length)\n",
    "# print(description2)\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6f7605",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# photo = extract_features(img_path[2], xception_model)\n",
    "# img = Image.open(img_path[2])\n",
    "# description3 = generate_desc(model, tokenizer, photo, max_length)\n",
    "# print(description3)\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244d693f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# photo = extract_features(img_path[3], xception_model)\n",
    "# img = Image.open(img_path[3])\n",
    "# description4 = generate_desc(model, tokenizer, photo, max_length)\n",
    "# print(description4)\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fb9718",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path = \"data\\\\images\\\\1355945307_f9e01a9a05.jpg\"\n",
    "# photo = extract_features(path, xception_model)\n",
    "# img = Image.open(path)\n",
    "# description5 = generate_desc(model, tokenizer, photo, max_length)\n",
    "# print(description5)\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36473372-38ad-4b5b-b960-1ae19aa5f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"data\\\\images\\\\free-images.jpg\"\n",
    "# photo = extract_features(path, xception_model)\n",
    "# img = Image.open(path)\n",
    "# description5 = generate_desc(model, tokenizer, photo, max_length)\n",
    "# print(description5)\n",
    "# plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
